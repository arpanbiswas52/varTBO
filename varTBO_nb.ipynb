{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Spectral optimization through BO with adaptive target setting\n",
    "\n",
    "- Problem domain expert - Rama Vasudevan\n",
    "- BO modeling -- Arpan Biswas\n",
    "\n",
    "# Problem Description\n",
    "\n",
    "Build a BO framework-\n",
    "\n",
    "- Here we have a image data, where X is the input location of the image\n",
    "\n",
    "- Each location in the image, we have a spectral data, from where user select if the data is good/bad (discrete choice). We take the means of all good spectral only to generate the target spectral.\n",
    "\n",
    "- At the mid of execution, the user also have option to either eliminate (or update the preferance) all the previously stored (if user suddenly find a desired spectral) spectral, and put more weights on new found spectral.\n",
    "\n",
    "- The goal is to build a optimization (BO) model where we adaptively sample towards region (in image) of good spectral, and find optimal location point closest to the current chosen target spectral (as per user voting).\n",
    "\n",
    "# Workflow: Run BO to find the optimal location for user-desired spectral as per voting\n",
    "\n",
    "- Here we vote the quality of spectral, for example: trying to get a greater loop area, with minimal noise and be as much towards theoritical hysteresis loop.\n",
    "\n",
    "- User interaction part:\n",
    "    1. Load Data. Currently, we have two dataset to choose: BEPFM and IV.\n",
    "    2. Initialize parameters. Select # of randomly selected starting samples (pre BO) and # of total BO iterations.\n",
    "    3. During the play (for each function evaluation)-\n",
    "\n",
    "        1. View spectral image. User have three rating options as 0, 1, 2: 0-Bad, 1-Good, 2-Very good.\n",
    "              1. If user rated as Good or Very Good, and user already have a current target spectral (user already selected at least one good/very good spectral in earlier iterations), another message pops up if the user want to update preference of new spectral Vs the current target spectral(Y or N).\n",
    "                1. If user choose \"Y\", user have option (any number between 0-1) to provide weights of new spectral. For example: If user provide 1 as input, the updated target spectral will be only the new chosen spectral and all the other earlier selected good spectral will be eliminated from target.\n",
    "                2. If user choose \"N\", as default the updated target spectral is the mean of all the earlier selected spectral and the new spectral.\n",
    "\n",
    "\n",
    "- After END OF PLAY, the code automatically save below data (serially) in optim.results.npy\n",
    "\n",
    "  1. Best locations, from evaluated samples and GP estimated\n",
    "\n",
    "  2. User rating of spectral at each evaluated locations in image\n",
    "\n",
    "  3. the matrices of evaluated location and the respective obj values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import gdown\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import numpy as np\n",
    "import random\n",
    "#import kornia as K\n",
    "#import kornia.metrics as metrics\n",
    "from PIL import Image\n",
    "from typing import Tuple\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import pylab as pl\n",
    "from IPython.display import clear_output\n",
    "import pdb\n",
    "from skimage.transform import resize\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "#import gpim\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.gridspec as gridspec\n",
    "from copy import deepcopy\n",
    "#import pyroved as pv\n",
    "#import atomai as aoi\n",
    "from typing import Union\n",
    "import pickle\n",
    "\n",
    "\n",
    "# Import GP and BoTorch functions\n",
    "import gpytorch as gpt\n",
    "from botorch.models import SingleTaskGP, ModelListGP\n",
    "#from botorch.models import gpytorch\n",
    "from botorch.fit import fit_gpytorch_model\n",
    "from botorch.models.gpytorch import GPyTorchModel\n",
    "from botorch.utils import standardize\n",
    "from gpytorch.distributions import MultivariateNormal\n",
    "from gpytorch.kernels import ScaleKernel, RBFKernel, MaternKernel\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from gpytorch.means import ConstantMean, LinearMean\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from botorch.acquisition import UpperConfidenceBound\n",
    "from botorch.optim import optimize_acqf\n",
    "from botorch.acquisition import qExpectedImprovement\n",
    "from botorch.acquisition import ExpectedImprovement\n",
    "from botorch.sampling import IIDNormalSampler\n",
    "from botorch.sampling import SobolQMCNormalSampler\n",
    "from gpytorch.likelihoods.likelihood import Likelihood\n",
    "from gpytorch.constraints import GreaterThan\n",
    "\n",
    "from botorch.generation import get_best_candidates, gen_candidates_torch\n",
    "from botorch.optim import gen_batch_initial_conditions\n",
    "\n",
    "from gpytorch.models import ExactGP\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from smt.sampling_methods import LHS\n",
    "from torch.optim import SGD\n",
    "from torch.optim import Adam\n",
    "from scipy.stats import norm\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "\n",
    "class SimpleCustomGP(ExactGP, GPyTorchModel):\n",
    "    _num_outputs = 1  # to inform GPyTorchModel API\n",
    "\n",
    "    def __init__(self, train_X, train_Y):\n",
    "        # squeeze output dim before passing train_Y to ExactGP\n",
    "        super().__init__(train_X, train_Y.squeeze(-1), GaussianLikelihood())\n",
    "        self.mean_module = ConstantMean()\n",
    "        #self.mean_module = LinearMean(train_X.shape[-1])\n",
    "        self.covar_module = ScaleKernel(\n",
    "            #base_kernel=MaternKernel(nu=2.5, ard_num_dims=train_X.shape[-1]),\n",
    "            base_kernel=RBFKernel(ard_num_dims=train_X.shape[-1]),\n",
    "        )\n",
    "        self.to(train_X)  # make sure we're on the right device/dtype\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "class play_varTBO():\n",
    "\n",
    "    def __init__(self):\n",
    "        print(\"List of dataset: bepfm or iv\")\n",
    "        input_args = str(input(\"Load dataset :Options: bepfm or iv \"))\n",
    "        print(\"#################################################################\")\n",
    "        print(\"Enter number of randomly selected starting samples: Recommended between 10 to 20\")\n",
    "        start_samples = float(input(\"Enter starting samples: \"))\n",
    "        print(\"Enter number of BO iterations: Recommended between 100 to 150\")\n",
    "        BO_iter = float(input(\"Enter BO iterations: \"))\n",
    "        sample_args = [start_samples, BO_iter]\n",
    "        print(\"#################################################################\")\n",
    "        print(\"Now start analysis: Loading Data\")\n",
    "        self.data_load(input_args, sample_args)\n",
    "\n",
    "    def func_obj(self, X, spec_norm, V, wcount_good, target_func, vote):\n",
    "        idx1 = int(X[0, 0])\n",
    "        idx2 = int(X[0, 1])\n",
    "        rf = 1\n",
    "        if (wcount_good == 0): #We dont find a good loop yet from all initial sampling and thus target is unknown\n",
    "            mse_spectral = torch.rand(1)*(1)+1 # Unif[1, 2] A sufficiently large value as we want to avoid selecting bad loops,\n",
    "            # When we have a target, we will recalculate again to get more accuract estimate\n",
    "            R = vote*rf\n",
    "\n",
    "        else:\n",
    "            #Calculate dissimilarity (mse) between target and ith loop shape\n",
    "            dev2_spectral = (target_func-spec_norm[idx1, idx2, :])**2\n",
    "            mse_spectral = torch.mean(dev2_spectral)\n",
    "            #Calculate reward as per voting, this will minimize the risk of similar function values of good and bad loop shape with similar mse\n",
    "            R = vote*rf\n",
    "\n",
    "        #This is the basic setting of obj func-- we can incorporate more info as per domain knowledge to improve\n",
    "        #Into maximization problem\n",
    "        obj = R - mse_spectral #Maximize reward and negative mse\n",
    "\n",
    "        return obj\n",
    "\n",
    "    def generate_targetobj(self, X, spec_norm, lowres_image, V, wcount_good, target_func):\n",
    "        #count_good= 0\n",
    "        idx1 = int(X[0, 0])\n",
    "        idx2 = int(X[0, 1])\n",
    "        #print(idx1, idx2)\n",
    "        #target_loop = torch.empty(loop_norm.shape[2])\n",
    "        if (wcount_good == 0):\n",
    "        # Figure for user choice\n",
    "            fig,ax=plt.subplots(ncols=2,figsize=(6,2))\n",
    "            ax[0].plot(V,spec_norm[idx1, idx2, :])\n",
    "            ax[0].set_title('loc:' +str(idx1) +\",\" + str(idx2), fontsize = 20)\n",
    "            #ax[0].axes.xaxis.set_visible(False)\n",
    "            #ax[0].axes.yaxis.set_visible(False)\n",
    "            ax[1].imshow(lowres_image.detach().numpy())\n",
    "            ax[1].plot(idx1, idx2, 'x', color=\"red\")\n",
    "            ax[1].axes.xaxis.set_visible(False)\n",
    "            ax[1].axes.yaxis.set_visible(False)\n",
    "\n",
    "            plt.show()\n",
    "\n",
    "        else:\n",
    "        # Figure for user choice\n",
    "            fig,ax=plt.subplots(ncols=3,figsize=(8,2))\n",
    "            ax[0].plot(V,spec_norm[idx1, idx2, :])\n",
    "            ax[0].set_title('loc:' +str(idx1) +\",\" + str(idx2), fontsize = 10)\n",
    "            #ax[0].axes.xaxis.set_visible(False)\n",
    "            #ax[0].axes.yaxis.set_visible(False)\n",
    "            ax[1].imshow(lowres_image.detach().numpy())\n",
    "            ax[1].plot(idx1, idx2, 'x', color=\"red\")\n",
    "            ax[1].axes.xaxis.set_visible(False)\n",
    "            ax[1].axes.yaxis.set_visible(False)\n",
    "            ax[2].plot(V,target_func)\n",
    "            ax[2].set_title('Current target function', fontsize = 10)\n",
    "            #ax[2].axes.xaxis.set_visible(False)\n",
    "            #ax[2].axes.yaxis.set_visible(False)\n",
    "            plt.show()\n",
    "\n",
    "        print(\"Rating: 0-Bad, 1-Good, 2-Very good\")\n",
    "        vote = float(input(\"enter rating: \"))\n",
    "        if(vote>0):\n",
    "            newspec_wt = 1\n",
    "            if ((wcount_good) > 0): #Only if we already have selected good spectral in early iterations\n",
    "                newspec_pref = str(input(\"Do you want to update preference to new spectral over prioir mean target (Y/N): \"))\n",
    "                if (newspec_pref == 'Y' or newspec_pref == 'y'):\n",
    "                    print(\"Provide weights between 0 and 1: 1 being all the weights to new spectral as new target\")\n",
    "                    newspec_wt = float(input(\"enter weightage: \"))\n",
    "                else:\n",
    "                    newspec_wt = 0.5\n",
    "            wcount_good =wcount_good + vote\n",
    "            target_func = (((1-newspec_wt)*target_func*(wcount_good-vote))\\\n",
    "                           + (newspec_wt*vote*spec_norm[idx1, idx2, :]))/(((wcount_good-vote)*(1-newspec_wt))\\\n",
    "                           + (vote*newspec_wt))\n",
    "        return vote, wcount_good, target_func\n",
    "\n",
    "    def optimize_hyperparam_trainGP(self, train_X, train_Y):\n",
    "        # Gp model fit\n",
    "\n",
    "        gp_surro = SimpleCustomGP(train_X, train_Y)\n",
    "        gp_surro = gp_surro.double()\n",
    "        gp_surro.likelihood.noise_covar.register_constraint(\"raw_noise\", GreaterThan(1e-1))\n",
    "        mll1 = ExactMarginalLogLikelihood(gp_surro.likelihood, gp_surro)\n",
    "        # fit_gpytorch_model(mll)\n",
    "        mll1 = mll1.to(train_X)\n",
    "        gp_surro.train()\n",
    "        gp_surro.likelihood.train()\n",
    "        ## Here we use Adam optimizer with learning rate =0.1, user can change here with different algorithm and/or learning rate for each GP\n",
    "        optimizer1 = Adam([{'params': gp_surro.parameters()}], lr=0.1) #0.01 set for BEPFM data, recommended to check the lr for any new data\n",
    "        #optimizer1 = SGD([{'params': gp_surro.parameters()}], lr=0.0001)\n",
    "\n",
    "        NUM_EPOCHS = 150\n",
    "\n",
    "        for epoch in range(NUM_EPOCHS):\n",
    "            # clear gradients\n",
    "            optimizer1.zero_grad()\n",
    "            # forward pass through the model to obtain the output MultivariateNormal\n",
    "            output1 = gp_surro(train_X)\n",
    "            # Compute negative marginal log likelihood\n",
    "            loss1 = - mll1(output1, gp_surro.train_targets)\n",
    "            # back prop gradients\n",
    "            loss1.backward(retain_graph=True)\n",
    "            # print last iterations\n",
    "            if (epoch + 1) > NUM_EPOCHS: #Stopping the print for now\n",
    "                print(\"GP Model trained:\")\n",
    "                print(\"Iteration:\" + str(epoch + 1))\n",
    "                print(\"Loss:\" + str(loss1.item()))\n",
    "                # print(\"Length Scale:\" +str(gp_PZO.covar_module.base_kernel.lengthscale.item()))\n",
    "                print(\"noise:\" + str(gp_surro.likelihood.noise.item()))\n",
    "\n",
    "\n",
    "            optimizer1.step()\n",
    "\n",
    "        gp_surro.eval()\n",
    "        gp_surro.likelihood.eval()\n",
    "        return gp_surro\n",
    "\n",
    "    def cal_posterior(self, gp_surro, test_X):\n",
    "        y_pred_means = torch.empty(len(test_X), 1)\n",
    "        y_pred_vars = torch.empty(len(test_X), 1)\n",
    "        t_X = torch.empty(1, test_X.shape[1])\n",
    "        for t in range(0, len(test_X)):\n",
    "            with torch.no_grad(), gpt.settings.max_lanczos_quadrature_iterations(32), \\\n",
    "                gpt.settings.fast_computations(covar_root_decomposition=False, log_prob=False,\n",
    "                                                          solves=True), \\\n",
    "                gpt.settings.max_cg_iterations(100), \\\n",
    "                gpt.settings.max_preconditioner_size(80), \\\n",
    "                gpt.settings.num_trace_samples(128):\n",
    "\n",
    "                    t_X[:, 0] = test_X[t, 0]\n",
    "                    t_X[:, 1] = test_X[t, 1]\n",
    "                    #t_X = test_X.double()\n",
    "                    y_pred_surro = gp_surro.posterior(t_X)\n",
    "                    y_pred_means[t, 0] = y_pred_surro.mean\n",
    "                    y_pred_vars[t, 0] = y_pred_surro.variance\n",
    "\n",
    "        return y_pred_means, y_pred_vars\n",
    "\n",
    "    def acqmanEI(self, y_means, y_vars, train_Y, ieval):\n",
    "\n",
    "\n",
    "        y_means = y_means.detach().numpy()\n",
    "        y_vars = y_vars.detach().numpy()\n",
    "        y_std = np.sqrt(y_vars)\n",
    "        fmax = train_Y.max()\n",
    "        fmax = fmax.detach().numpy()\n",
    "        best_value = fmax\n",
    "        EI_val = np.zeros(len(y_vars))\n",
    "        Z = np.zeros(len(y_vars))\n",
    "        eta = 0.01\n",
    "\n",
    "        for i in range(0, len(y_std)):\n",
    "            if (y_std[i] <=0):\n",
    "                EI_val[i] = 0\n",
    "            else:\n",
    "                Z[i] =  (y_means[i]-best_value-eta)/y_std[i]\n",
    "                EI_val[i] = (y_means[i]-best_value-eta)*norm.cdf(Z[i]) + y_std[i]*norm.pdf(Z[i])\n",
    "\n",
    "        # Eliminate evaluated samples from consideration to avoid repeatation in future sampling\n",
    "        EI_val[ieval] = -1\n",
    "        acq_val = np.max(EI_val)\n",
    "        acq_cand = [k for k, j in enumerate(EI_val) if j == acq_val]\n",
    "        #print(acq_val)\n",
    "        return acq_cand, acq_val, EI_val\n",
    "\n",
    "    def normalize_get_initialdata_KL(self, X, fix_params, num, m):\n",
    "\n",
    "        X_feas = torch.empty((X.shape[1]**X.shape[0], X.shape[0]))\n",
    "        k=0\n",
    "        spec_norm, lowres_image, V  = fix_params[0], fix_params[1], fix_params[2]\n",
    "        for t1 in range(0, X.shape[1]):\n",
    "            for t2 in range(0, X.shape[1]):\n",
    "                X_feas[k, 0] = X[0, t1]\n",
    "                X_feas[k, 1] = X[1, t2]\n",
    "                k=k+1\n",
    "\n",
    "        X_feas_norm = torch.empty((X_feas.shape[0], X_feas.shape[1]))\n",
    "        #train_X = torch.empty((len(X), num))\n",
    "        #train_X_norm = torch.empty((len(X), num))\n",
    "        train_Y = torch.empty((num, 1))\n",
    "        pref = torch.empty((num, 1))\n",
    "\n",
    "\n",
    "        # Normalize X\n",
    "        for i in range(0, X_feas.shape[1]):\n",
    "            X_feas_norm[:, i] = (X_feas[:, i] - torch.min(X_feas[:, i])) / (torch.max(X_feas[:, i]) - torch.min(X_feas[:, i]))\n",
    "\n",
    "\n",
    "\n",
    "        # Select starting samples randomly as training data\n",
    "        np.random.seed(0)\n",
    "        idx = np.random.randint(0, len(X_feas), num)\n",
    "        train_X = X_feas[idx]\n",
    "        train_X_norm = X_feas_norm[idx]\n",
    "        #print(train_X)\n",
    "        #print(train_X_norm)\n",
    "\n",
    "        #Evaluate initial training data\n",
    "        x = torch.empty((1,2))\n",
    "        # First generate target loop, based on initial training data\n",
    "        wcount_good= 0\n",
    "        target_func = torch.zeros(spec_norm.shape[2])\n",
    "        for i in range(0, num):\n",
    "            x[0, 0] = train_X[i, 0]\n",
    "            x[0, 1] = train_X[i, 1]\n",
    "            print(\"#################################################################\")\n",
    "            print(\"Sample #\" + str(m + 1))\n",
    "            pref[i, 0], wcount_good, target_func = self.generate_targetobj(x, spec_norm, lowres_image, V, wcount_good, target_func)\n",
    "            m = m + 1\n",
    "\n",
    "        # Once target loop is defined (unless are loops are selected bad by user), we compute the obj\n",
    "        for i in range(0, num):\n",
    "            x[0, 0] = train_X[i, 0]\n",
    "            x[0, 1] = train_X[i, 1]\n",
    "\n",
    "            #print(\"Function eval #\" + str(m + 1))\n",
    "\n",
    "            train_Y[i, 0] = self.func_obj(x, spec_norm, V, wcount_good, target_func, pref[i, 0])\n",
    "            #m = m + 1\n",
    "        #print(pref)\n",
    "        #print(train_Y)\n",
    "        var_params = [wcount_good, pref, target_func]\n",
    "\n",
    "        return X_feas, X_feas_norm, train_X, train_X_norm, train_Y, var_params, idx, m\n",
    "\n",
    "    def augment_newdata_KL(self, acq_X, acq_X_norm, train_X, train_X_norm, train_Y, fix_params, var_params, m):\n",
    "        spec_norm, lowres_image, V  = fix_params[0], fix_params[1], fix_params[2]\n",
    "        wcount_good, pref, target_func = var_params[0], var_params[1], var_params[2]\n",
    "\n",
    "        nextX = acq_X\n",
    "        nextX_norm = acq_X_norm\n",
    "        #train_X_norm = torch.cat((train_X_norm, nextX_norm), 0)\n",
    "        #train_X_norm = train_X_norm.double()\n",
    "        train_X_norm = torch.vstack((train_X_norm, nextX_norm))\n",
    "        train_X = torch.vstack((train_X, nextX))\n",
    "\n",
    "        p = torch.empty(1, 1)\n",
    "        x = torch.empty((1,2))\n",
    "        x[0, 0] = train_X[-1, 0]\n",
    "        x[0, 1] = train_X[-1, 1]\n",
    "\n",
    "        print(\"Sample #\" + str(m + 1))\n",
    "        #Vote for new spectral sample and update(or not) target spectral function\n",
    "        p[0, 0], wcount_good, target_func = self.generate_targetobj(x, spec_norm, lowres_image, V, wcount_good, target_func)\n",
    "        pref = torch.vstack((pref, p)) #Augment pref matrix\n",
    "\n",
    "        if (p[0, 0] == 0):\n",
    "          # If target loop is not updated, we eval for the new samples only as the func value for others remain the same\n",
    "\n",
    "            next_feval = torch.empty(1, 1)\n",
    "            next_feval[0, 0] = self.func_obj(x, spec_norm, V, wcount_good, target_func, pref[-1, 0])\n",
    "            train_Y = torch.vstack((train_Y, next_feval))\n",
    "        else:\n",
    "            # If target loop is updated, we reevaluate the obj of old spectral samples and eval for the new samples\n",
    "            train_Y = torch.empty((train_X.shape[0], 1))\n",
    "            for i in range(0, train_X.shape[0]):\n",
    "                x[0, 0] = train_X[i, 0]\n",
    "                x[0, 1] = train_X[i, 1]\n",
    "                train_Y[i, 0] = self.func_obj(x, spec_norm, V, wcount_good, target_func, pref[i, 0])\n",
    "\n",
    "\n",
    "        #print(pref)\n",
    "        #print(train_Y)\n",
    "        var_params = [wcount_good, pref, target_func]\n",
    "        m = m + 1\n",
    "        return train_X, train_X_norm, train_Y, var_params, m\n",
    "\n",
    "    def plot_iteration_results(self, train_X, train_Y, test_X, y_pred_means, y_pred_vars, fix_params, i):\n",
    "        spec_norm, lowres_image, V  = fix_params[0], fix_params[1], fix_params[2]\n",
    "        pen = 10**0\n",
    "        #Best solution among the evaluated data\n",
    "\n",
    "        loss = torch.max(train_Y)\n",
    "        ind = torch.argmax(train_Y)\n",
    "        X_opt = torch.empty((1,2))\n",
    "        #X_opt = train_X[ind, :]\n",
    "        X_opt[0, 0] = train_X[ind, 0]\n",
    "        X_opt[0, 1] = train_X[ind, 1]\n",
    "\n",
    "\n",
    "        # Best estimated solution from GP model considering the non-evaluated solution\n",
    "\n",
    "        loss = torch.max(y_pred_means)\n",
    "        ind = torch.argmax(y_pred_means)\n",
    "        X_opt_GP = torch.empty((1,2))\n",
    "        #X_opt = train_X[ind, :]\n",
    "        X_opt_GP[0, 0] = test_X[ind, 0]\n",
    "        X_opt_GP[0, 1] = test_X[ind, 1]\n",
    "\n",
    "        #Objective map\n",
    "        plt.figure()\n",
    "\n",
    "        fig,ax=plt.subplots(ncols=3,figsize=(8,2))\n",
    "        a= ax[0].imshow(lowres_image.detach().numpy(), origin=\"lower\")\n",
    "        ax[0].scatter(train_X[:,0], train_X[:,1], marker='o', c='g')\n",
    "        ax[0].scatter(X_opt[0, 0], X_opt[0, 1], marker='x', c='r')\n",
    "        ax[0].scatter(X_opt_GP[0, 0], X_opt_GP[0, 1], marker='o', c='r')\n",
    "        ax[0].axes.xaxis.set_visible(False)\n",
    "        ax[0].axes.yaxis.set_visible(False)\n",
    "\n",
    "\n",
    "        a = ax[1].scatter(test_X[:,0], test_X[:,1], c=y_pred_means/pen, cmap='viridis', linewidth=0.2)\n",
    "        ax[1].scatter(train_X[:,0], train_X[:,1], marker='o', c='g')\n",
    "        ax[1].scatter(X_opt[0, 0], X_opt[0, 1], marker='x', c='r')\n",
    "        ax[1].scatter(X_opt_GP[0, 0], X_opt_GP[0, 1], marker='o', c='r')\n",
    "        divider = make_axes_locatable(ax[1])\n",
    "        cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "        fig.colorbar(a, cax=cax, orientation='vertical')\n",
    "        ax[1].set_title('Objective (GP mean) map', fontsize=10)\n",
    "        ax[1].axes.xaxis.set_visible(False)\n",
    "        ax[1].axes.yaxis.set_visible(False)\n",
    "        #ax[1].colorbar(a)\n",
    "\n",
    "        b = ax[2].scatter(test_X[:,0], test_X[:,1], c=y_pred_vars/(pen**2), cmap='viridis', linewidth=0.2)\n",
    "        divider = make_axes_locatable(ax[2])\n",
    "        cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "        fig.colorbar(b, cax=cax, orientation='vertical')\n",
    "        ax[2].set_title('Objective (GP var) map', fontsize=10)\n",
    "        ax[2].axes.xaxis.set_visible(False)\n",
    "        ax[2].axes.yaxis.set_visible(False)\n",
    "        #ax[2].colorbar(b)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "        return X_opt, X_opt_GP\n",
    "\n",
    "    def BO_vartarget(self, X, fix_params, num_start, N):\n",
    "        num = num_start\n",
    "        m = 0\n",
    "        # Initialization: evaluate few initial data normalize data\n",
    "        test_X, test_X_norm, train_X, train_X_norm, train_Y, var_params, idx, m = \\\n",
    "            self.normalize_get_initialdata_KL(X, fix_params, num, m)\n",
    "\n",
    "        print(\"#################################################################\")\n",
    "        print(\"Initial evaluation complete. Start BO\")\n",
    "        print(\"#################################################################\")\n",
    "        ## Gp model fit\n",
    "        # Calling function to fit and optimizize Hyperparameter of Gaussian Process (using Adam optimizer)\n",
    "        # Input args- Torch arrays of normalized training data, parameter X and objective eval Y\n",
    "        # Output args- Gaussian process model lists\n",
    "        gp_surro = self.optimize_hyperparam_trainGP(train_X_norm, train_Y)\n",
    "\n",
    "        for i in range(1, N + 1):\n",
    "            # Calculate posterior for analysis for intermidiate iterations\n",
    "            y_pred_means, y_pred_vars = self.cal_posterior(gp_surro, test_X_norm)\n",
    "            if ((i == 1) or ((i % 10) == 0)):\n",
    "                # Plotting functions to check the current state exploration and Pareto fronts\n",
    "                X_eval, X_GP = self.plot_iteration_results(train_X, train_Y, test_X, y_pred_means, y_pred_vars, fix_params, i)\n",
    "\n",
    "            #print(idx)\n",
    "            acq_cand, acq_val, EI_val = self.acqmanEI(y_pred_means, y_pred_vars, train_Y, idx)\n",
    "            val = acq_val\n",
    "            ind = np.random.choice(acq_cand) # When multiple points have same acq values\n",
    "            idx = np.hstack((idx, ind))\n",
    "\n",
    "\n",
    "            ################################################################\n",
    "            ## Find next point which maximizes the learning through exploration-exploitation\n",
    "            if (i == 1):\n",
    "                val_ini = val\n",
    "            # Check for convergence\n",
    "            if ((val) < 0):  # Stop for negligible expected improvement\n",
    "                print(\"Model converged due to sufficient learning over search space \")\n",
    "                break\n",
    "            else:\n",
    "                nextX = torch.empty((1, len(X)))\n",
    "                nextX_norm = torch.empty(1, len(X))\n",
    "                nextX[0,:] = test_X[ind, :]\n",
    "                nextX_norm [0, :] = test_X_norm[ind, :]\n",
    "\n",
    "                # Evaluate true function for new data, augment data\n",
    "                train_X, train_X_norm, train_Y, var_params, m =\\\n",
    "                 self.augment_newdata_KL(nextX, nextX_norm, train_X, train_X_norm, train_Y, fix_params, var_params, m)\n",
    "\n",
    "                # Gp model fit\n",
    "                # Updating GP with augmented training data\n",
    "                gp_surro = self.optimize_hyperparam_trainGP(train_X_norm, train_Y)\n",
    "\n",
    "        ## Final posterior prediction after all the sampling done\n",
    "\n",
    "        if (i == N):\n",
    "            print(\"#################################################################\")\n",
    "            print(\"Max. sampling reached, model stopped\")\n",
    "            print(\"#################################################################\")\n",
    "\n",
    "        #Optimal GP learning\n",
    "        gp_opt = gp_surro\n",
    "        # Posterior calculation with converged GP model\n",
    "        y_pred_means, y_pred_vars = self.cal_posterior(gp_opt, test_X_norm)\n",
    "        # Plotting functions to check final iteration\n",
    "        X_opt, X_opt_GP = self.plot_iteration_results(train_X, train_Y, test_X, y_pred_means, y_pred_vars, fix_params, i)\n",
    "        explored_data = [train_X, train_Y]\n",
    "        final_GP_estim = [y_pred_means, y_pred_vars]\n",
    "        user_votes = var_params[1]\n",
    "        optim_results = [X_opt, X_opt_GP, user_votes, explored_data]\n",
    "\n",
    "        #Save few data\n",
    "        np.save(\"optim_results\", optim_results)\n",
    "\n",
    "\n",
    "        return  X_opt, X_opt_GP, var_params, explored_data, final_GP_estim\n",
    "\n",
    "    def data_load(self, input_args, sample_args):\n",
    "\n",
    "        if (input_args == 'BEPFM' or input_args == 'bepfm'):\n",
    "            #!pip install -U gdown\n",
    "\n",
    "            #!gdown \"https://drive.google.com/uc?id=1PoklVNuIwhAMbig0LvxW-zAK82et_aL-\"\n",
    "            #!gdown \"https://drive.google.com/uc?id=11v5wHmMT0xYcxAYejjxLKz1QARST_JRn\"\n",
    "            #!gdown \"https://drive.google.com/uc?id=1jQZIW7uiNV0J-mDl1yNylhNA6rRFnMM4\"\n",
    "\n",
    "            gdown.download(\"https://drive.google.com/uc?id=1PoklVNuIwhAMbig0LvxW-zAK82et_aL-\", \"loop_mat.npy\", quiet=True)\n",
    "            gdown.download(\"https://drive.google.com/uc?id=11v5wHmMT0xYcxAYejjxLKz1QARST_JRn\", \"dc_vec.npy\", quiet=True)\n",
    "            gdown.download(\"https://drive.google.com/uc?id=1jQZIW7uiNV0J-mDl1yNylhNA6rRFnMM4\", \"bepfm_image.npy\", quiet=True)\n",
    "\n",
    "\n",
    "            #boptim_results = np.load(\"boptim_results.npy\", allow_pickle=True)\n",
    "            loop_mat = np.load(\"loop_mat.npy\")\n",
    "            dc_vec = np.load(\"dc_vec.npy\")\n",
    "            bepfm_image = np.load(\"bepfm_image.npy\")\n",
    "\n",
    "            n_spectral = 2 # Consider the final loop measurement\n",
    "            loop_mat_grid = np.reshape(loop_mat,(60, 60, loop_mat.shape[1], loop_mat.shape[2]))\n",
    "            loop = loop_mat_grid[:, :, :, n_spectral]\n",
    "            #print(loop.shape)\n",
    "\n",
    "            #Tranform the image data to map with spectral data\n",
    "            grid_dim = loop.shape[1]\n",
    "            lowres_image = resize(bepfm_image, (grid_dim, grid_dim))\n",
    "            #print(bepfm_lowres_image.shape)\n",
    "\n",
    "            #Consider single sweep of voltage to generate hysteresis loop\n",
    "            l_vsweep= loop.shape[2]\n",
    "            V= dc_vec[:l_vsweep]\n",
    "            #print(V.shape)\n",
    "\n",
    "            #Normalize loop data (avoiding drift in data)\n",
    "            loop_norm = np.zeros((loop.shape))\n",
    "            for i in range(0, loop.shape[0]):\n",
    "              for j in range(0, loop.shape[1]):\n",
    "                loop_norm[i, j, :] = (loop[i,j,:]- np.mean(loop[i,j,:]))*1e4\n",
    "\n",
    "            spec = loop_norm\n",
    "            #print(loop.shape, loop_norm.shape)\n",
    "        elif (input_args == 'IV' or input_args == 'iv'):\n",
    "            #!pip install -U gdown\n",
    "            #!gdown \"https://drive.google.com/uc?id=1l9-i36puXUcGXQMGSgVjci_x-aNC4bzp\"\n",
    "            #!gdown \"https://drive.google.com/uc?id=1-YMtG0kjypcmVvJDJwaJ1QUf5Ar32uj-\"\n",
    "\n",
    "            gdown.download(\"https://drive.google.com/uc?id=1l9-i36puXUcGXQMGSgVjci_x-aNC4bzp\", \"cur_mat_raw.npy\", quiet=True)\n",
    "            gdown.download(\"https://drive.google.com/uc?id=1-YMtG0kjypcmVvJDJwaJ1QUf5Ar32uj-\", \"cur_mat_vdc.npy\", quiet=True)\n",
    "\n",
    "\n",
    "            iv_dat = np.load(\"cur_mat_raw.npy\")\n",
    "            iv_dat = iv_dat\n",
    "            dc_vec = np.load(\"cur_mat_vdc.npy\")\n",
    "            iv_dat = iv_dat.reshape(50,50,iv_dat.shape[1])*1e11\n",
    "            V= dc_vec\n",
    "            #print(iv_dat.shape)\n",
    "            #print(V.shape)\n",
    "\n",
    "            #Generate the grid param space (taking means of spectral values at each grid location)\n",
    "            lowres_image = np.mean(iv_dat, axis=2)\n",
    "            lowres_image = np.transpose(lowres_image)\n",
    "            #print(iv_lowres_image.shape)\n",
    "            #plt.figure()\n",
    "            #plt.imshow(iv_lowres_image, origin=\"lower\")\n",
    "            #plt.colorbar()\n",
    "\n",
    "            #Normalize loop data (avoiding drift in data)\n",
    "            spec = np.zeros((iv_dat.shape))\n",
    "            for i in range(0, iv_dat.shape[0]):\n",
    "              for j in range(0, iv_dat.shape[1]):\n",
    "                spec[i, j, :] = (iv_dat[i,j,:]- np.mean(iv_dat[i,j,:]))\n",
    "\n",
    "        else:\n",
    "            print(\"Error: Choose correct file name: Check spelling\")\n",
    "            sys.exit()\n",
    "\n",
    "\n",
    "        #tranform data into torch\n",
    "        spec = torch.from_numpy(spec)\n",
    "        V = torch.from_numpy(V)\n",
    "        lowres_image= torch.from_numpy(lowres_image)\n",
    "\n",
    "        grid_x1 = torch.arange(0, lowres_image.shape[0])\n",
    "        grid_x2 = torch.arange(0, lowres_image.shape[1])\n",
    "\n",
    "        X= torch.vstack((grid_x1, grid_x2))\n",
    "\n",
    "        #Fixed parameters of VAE model\n",
    "        fix_params = [spec, lowres_image, V]\n",
    "        num_start = int(sample_args[0])\n",
    "        N = int(sample_args[1])\n",
    "        print(\"Complete Data Loading...\")\n",
    "        print(\"#################################################################\")\n",
    "\n",
    "        X_opt, X_opt_GP, var_params, explored_locs, final_GP_estim = self.BO_vartarget(X, fix_params, num_start, N)\n",
    "\n",
    "\n",
    "        #Display\n",
    "        print(\"#################################################################\")\n",
    "        print(\"Display best spectrals\")\n",
    "        plt.figure()\n",
    "\n",
    "        fig,ax=plt.subplots(ncols=2,figsize=(7,2))\n",
    "\n",
    "        #Optimal from estimated GP\n",
    "        idx1 = int(X_opt[0, 0])\n",
    "        idx2 = int(X_opt[0, 1])\n",
    "        ax[0].plot(V, spec[idx1,idx2,:])\n",
    "        ax[0].set_title('Best loc (GP Estimated):' +str(idx1) +\",\" + str(idx2), fontsize = 10)\n",
    "\n",
    "        #Optimal from evaluated samples\n",
    "        idx1 = int(X_opt_GP[0, 0])\n",
    "        idx2 = int(X_opt_GP[0, 1])\n",
    "        ax[1].plot(V, spec[idx1,idx2,:])\n",
    "        ax[1].set_title('Best loc (evaluated):' +str(idx1) +\",\" + str(idx2), fontsize = 10)\n",
    "        plt.show()\n",
    "        print(\"#################################################################\")\n",
    "        print(\"End of Analysis\")\n",
    "\n",
    "play_varTBO()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
